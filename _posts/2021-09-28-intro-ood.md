---
layout: distill
title: Out-of-distribution detection in deep neural networks
description: This post provides a brief introduction to the problem of out-of-distribution examples. Examples will be provided to make things as clear as possible.

date: 2021-09-28 0:00:00-0400


authors:
   - name: Gabriel Raya

bibliography: 2018-12-22-distill.bib
---

## Introduction

<p align="justify">
Nowadays, we see deep neural networks (DNNs) used almost anywhere in tasks that seems to outperform human intelligence such as image recognition, speech recognition, product recommendation, among many others. Though a single input drawn from a different distribution, or even the same input but slightly modified (i.e., perturbed with noise <d-cite key="szegedy2013intriguing"></d-cite>) can cause the model to give completely inaccurate predictions. For example, a model trained on MNIST with 97% accuracy (Fig.1 upper row) will misclassify inputs from FashionMNIST (Fig. 1 lower row).  
</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/ood/ood_problem.PNG">
    </div>
</div>
<div class="caption">
    Figure 1. A fully connected neural network train on 60,000 images from the MNIST dataset with a 97% of accuracy on the test set (10,000 images). (Upper row) Predictions for samples from MNIST (in-distribution). (Lower row) Predictions for samples from FashionMNIST (out-of-distribution).  <a href="https://github.com/gabrielraya/out-of-distribution-detection/blob/main/Intro_to_OOD.ipynb" target="blank">Colab Code</a>

    <a href="https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>
</div>

<p align="justify">
The model doesn't how to say <i>I don't know!</i> when the model is unsure about its predictions.
This failure in DNNs is known as the out-of-distribution problem and it represents a critical problem for AI safety. Formally, the out-of-distribution problem is the following:</p>

<p align="justify">
<i>Given a model $M(X, \theta)$ trained on $X=\{x_1, ..., x_n\}$ samples from the same distribution $p(X)$, $X \sim p(X)$, we want to determine if a new sample  $x^*$ was drawn from the training distribution $p(X)$ or from other distribution $q(X) \ne p(X)$.</i>
</p>




<!-- A similar situation like this in real life could be a self-driving car designe  d in California driving in the streets of Amsterdam. *How does the system should behave now that the  driving rules might differ from the ones in California, to the size and structure of the streets, the bike roads, etc.?*. All these changes represent a change in distribution, and out-of-distribution detection is exactly about that, detecting those changes in distributions (at testing time) to provide more robust systems in real world settings. -->


## Methods

### Supervised methods



### Unsupervised methods


## thesis

What we really want is an algorithm that deals with the out-of-distribution generalization problem using unsupervised learning.

Unsupervised out-of-distribution generalization, what do we need?
- Dynamics
-

## Maximum Likelihood and the i.i.d. assumption

Despite huge success, deep neural networks are still not reliable enough to work under critical conditions. As they are susceptible to data drawn from a distribution different to that of the training data, mistakenly assigning high confident predictions to such out-of-distribution (OOD) inputs. A natural approach to overcome this issue is to use a deep generative model (DGM) to reconstruct the probability density of the training data and use the likelihood estimates to find whether a new data point $x^*$ comes from the training data distribution. However, in this thesis, we found that DGMs are susceptible to OOD inputs, as their likelihood estimates are poorly calibrated. This happened when we tested a variational autoencoder (VAE) over several image data sets. To mitigate this problem, we propose using a Bayesian Variational Autoencoder (BVAE) and an Ensemble of VAEs to robustify the OOD detection score by estimating the \textit{epistemic} uncertainty of the likelihood model.



We can learn score functions (gradients of log probability density functions) on a large number of noise-perturbed data distributions, and then generate samples by Langevin-type sampling. The resulting generative models, often called score-based generative models (or diffusion probabilistic models), has several important advantages over existing model families: GAN-level sample quality without adversarial training, flexible model architectures, exact log-likelihood computation, and inverse problem solving without re-training models. In this blog post, we will show you in more detail the intuition, basic concepts, and potential applications of score-based generative models.




Over the last few years, deep neural networks (DNNs) have made remarkable progress in many fields, ranging from applied to fundamental research, e.g., physics, biology, health, computer vision, autonomous driving cars, among many others. Despite this considerable success, DNNs are still not reliable enough to work under dataset shift, a common phenomenon present in machine learning when the conditions in which the model is trained are entirely different to those in which the model is used, representing a critical problem to AI safety.

Imagine a self-driving car DNN trained in California, and later on tested in the Netherlands. The conditions have changed, and the model wouldn't recognized that the bikes have preference most of the time!. To illustrate this, let's consider the example shown in Figure \ref{fig:ood-example}. A classifier is trained on samples $X=\{x_1, ..., x_n\}$  from the CIFAR10 \cite{Krizhevsky09learningmultiple} dataset. We assume that these samples are drawn independently from the same distribution $p(X)$. Since the model is only exposed to samples from $p(X)$ at training time, these are also known as in-distribution samples. By adjusting the configuration on the model parameters, the model learns to classify the ten different classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)  with a 94\% accuracy on the test set. What if a user decides to test the model with a new input $x^*$ from a distribution $q(X) \ne p(X)$, e.g., an image from the SVHN dataset?. These samples are also known as \textit{out-of-distribution} (OOD) samples since they come from a distribution different than the training distribution. Figure \ref{fig:ood-example} shows how the model not only assigns incorrect predictions to OOD samples,  but it does so with high probability. For example, Figure \ref{fig:intro-ood} shows how an OOD input $x^*$, with the corresponding label `3`, is incorrectly classified as `bird` with maximum probability.  This failure in DNNs is known as the out-of-distribution problem. OOD detection allows us to measure how a model generalizes to domain shift, detecting if the \textit{model knows what it knows} \cite{Lakshminarayanan2017SimpleAS}, a fundamental assessment to safety-critical applications.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/ood/dl_fails_in_data.png">
    </div>
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/ood/dl_fails_ood_data.png">
    </div>
</div>

his theme supports rendering beautiful math in inline and display modes using [MathJax 3](https://www.mathjax.org/){:target="\_blank"} engine. You just need to surround your math expression with `$$`, like `$$ E = mc^2 $$`. If you leave it inside a paragraph, it will produce an inline expression, just like $$ E = mc^2 $$.

To use display mode, again surround your expression with `$$` and place it as a separate paragraph. Here is an example:

$$
\sum_{k=1}^\infty |\langle x, e_k \rangle|^2 \leq \|x\|^2
$$

You can also use `\begin{equation}...\end{equation}` instead of `$$` for display mode math.
MathJax will automatically number equations:

\begin{equation}
\label{eq:caushy-shwarz}
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
\end{equation}

and by adding `\label{...}` inside the equation environment, we can now refer to the equation using `\eqref`.

Note that MathJax 3 is [a major re-write of MathJax](https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html){:target="\_blank"} that brought a significant improvement to the loading and rendering speed, which is now [on par with KaTeX](http://www.intmath.com/cg5/katex-mathjax-comparison.php){:target="\_blank"}.
## Thoughts
How humans achieve out-of-distribution generalization?

For example, let's imagine the situation that you are a non Dutch speaker and decide to move to the Netherlands. Now, you are at the store and suddenly starts talking to you in Dutch, if you are not confident enough about what the person is talking to you, you might say something like <i>Sorry, do you speak English?, I don't understand!</i>.

But how this process happen?
