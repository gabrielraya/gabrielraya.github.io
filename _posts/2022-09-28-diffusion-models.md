---
layout: distill
title: Diffusion models
description: An overview of diffusion models (work in progress).

date: 2022-07-29 0:00:00-0400
keywords:
  - generative models
  - diffusion models
  - score-based generative models
description: An overview of diffusion models (work in progress).

authors:
   - name: Gabriel Raya
     url: "https://gabrielraya.com/"

bibliography: 2018-12-22-distill.bib
comments: true
# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.



---

<p align="justify">
If you are reading this post, then you may have heard that diffusion models are kind of the <em>big thing</em> for generating high-quality image data. Simply see below.
</p>

<div class="container">
  <div class="row">
      <div class="col-3" >
          <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/diffusion/sample_quality_2.PNG" style="width: 100%;" class="center">
      </div>
      <div class="col-3">
          <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/diffusion/sample_quality.PNG"
          style="width: 100%;">
      </div>
      <div class="col-3">
          <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/diffusion/sample_quality_4.PNG"
          style="width: 100%;">
      </div>
      <div class="col-3">
          <img class="img-fluid rounded z-depth-1" src="{{ site.baseurl }}/assets/img/diffusion/sample_quality_5.PNG"
          style="width: 100%;">
      </div>
  </div>
  <figcaption class="figure-caption text-center">Fig. 1. Samples generated by a Diffusion model (Song.et.al.<d-cite key="song2020score"></d-cite> ). </figcaption><br>
</div>

<p align="justify">
However, grasping diffusion models may seem initially daunting, and you may easily get confused by terms such as denoising diffusion and score-based generative models. Therefore, this blog post aims to provide a big picture of diffusion models, whether you prefer to call them diffusion, denoising diffusion or score-based diffusion models. All of them have the same roots, the diffusion process, so I will refer to them as Diffusion models and make it explicit when a singular idea comes from a particular approach.
</p>


For more in-depth information, I will always suggest reading the original papers:

- [Deep Unsupervised Learning using Nonequilibrium Thermodynamics, 2015](https://arxiv.org/abs/1503.03585).
- [Denoising Diffusion Probabilistic Models, 2020](https://arxiv.org/abs/2006.11239)


I also recommend reading the following blog post, from which some information I have added here to provide a complete picture.  

- [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)

<p align="justify">
Before starting, I would like to recall that modeling high-dimensional distributions where training, sampling, and evaluating probabilities is tractable is not easy. As a result, approximation techniques such as variational inference and sampling methods have been developed over the years. Now, with Diffusion models, we can do that and more!.
</p>




## What are diffusion models


<p align="justify">
Diffusion Models  are probabilistic models that are capable to model high-dimensional distributions using two diffusion processes: 1) a   <em>forward diffusion process</em> maps data to a noise distribution, i.e., an isotropic Gaussian, and 2) a <em>reverse diffusion process</em> that moves samples from the noise distribution back to the data distribution. The essential idea in diffusion models, inspired by nonequilibrium statistical physics and introduced by Sohl-Dickstein et. al. <d-cite key="sohl2015deep"></d-cite>, is to <b>systematically</b> and slowly destroy structure in a data distribution through an iterative diffusion process. Then learn a reverse diffusion process that restores structure in data.  In this way, learning in a diffusion model consists on estimating small perturbations which is more tractable that approximating the partition function.<br><br>

Stochastic process can be time-continuous or discrete. Because a diffusion process is a stochastic process, diffusion models can be time-continuous or discrete. I will start explaining time-discrete diffusion models.

</p>





### Discrete Diffusion models
Lets assume that our dataset consists of $N$ i.i.d inputs $$\{\mathbf{x}_0^n\}_{n=0}^N \sim q_0(\mathbf{x})$$ sampled from an unknown distribution $$q_0(\mathbf{x}_0)$$, where the lower-index is used to denoted the time-dependency in the diffusion process. The goal is to find a parametric model $p_{\theta}(\\mathbf{x}_0) \approx q_0(\\mathbf{x}_0)$ using a reversible diffusion process that evolves over a discrete time variable $t\in[0,T]$


<div style="align: left; text-align:center;">
        <img class="img-fluid  " src="{{ site.baseurl }}/assets/img/diffusion/generative_markov_chain.png" style="width: 700px;">
        <figcaption class="figure-caption text-center">Figure 2. The directed graphical model for a discrete diffusion model is represented by a Markov Chain. The forward/reverse diffusion process systematically and  slowly adds/removes noise.</figcaption>
</div><br>

#### Forward trajectory

<p align="justify">
 The forward diffusion process will systematically perturbed the input data $\mathbf{x}_0$ using a perturbation kernel $q(\mathbf{x}_t \vert \mathbf{x}_{t-1})$ over time $t\in[0,T]$, gradually converting $q_0(\mathbf{x}_0)$ into a simple known distribution $q_T$ (e.g. a Gaussian) distribution. For example, a input $\mathbf{x}_0$, like the one shown in figure 1 (going from right to left), gradually loses its distinguishable features as the step $t$ becomes larger. Eventually when $T \to \infty$, $\mathbf{x}_T$ is equivalent to an isotropic Gaussian distribution. Figure 1 shows with dotted line a forward diffusion step.  This <b>forward trajectory</b> or <b>forward diffusion process</b> is represented by a Markov chain.  The stationary distribution is chose by design and so the forward process does not have learnable parameters. For example, Sohl-Dickstein et. al. <d-cite key="sohl2015deep"></d-cite> proposed the following perturbation kernel such that the stationary distribution is Isotropic Gaussian.
</p>


$$
\begin{equation}
\label{eq:transition_kernel}
q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I})
\end{equation}

$$

<p align="justify">
where $\beta_t$ is the variance schedule, a sequence of positive noise scales such that $0 < \beta_1, \beta_2, ..., \beta_T < 1$.
A nice property of the above process is that we can sample $\mathbf{x}_t$ at any arbitrary time step $t$ in a closed form using <a href="https://lilianweng.github.io/posts/2018-08-12-vae/#reparameterization-trick">reparameterization trick</a>. Let $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$, and now $q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_{t-1}, 1-\alpha_t\mathbf{I})$
</p>

<div>
$$
\begin{aligned}
\mathbf{x}_1 &= \sqrt{\alpha_1}\mathbf{x}_{0} + \sqrt{1 - \alpha_1}\mathbf{\epsilon}_{0}  \quad\quad\quad\quad\quad\quad\quad\quad\quad \text{ ;where } \mathbf{\epsilon}_{0}, \mathbf{\epsilon}_{1}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\
\mathbf{x}_2 &= \sqrt{\alpha_2}\mathbf{x}_{1} + \sqrt{1 - \alpha_2}\mathbf{\epsilon}_{1}\\
&= \sqrt{\alpha_2}\left(\sqrt{\alpha_1}\mathbf{x}_{0} + \sqrt{1 - \alpha_1}\mathbf{\epsilon}_{0}\right) + \sqrt{1 - \alpha_2}\mathbf{\epsilon}_{1}\\
&= \sqrt{\alpha_2\alpha_1}\mathbf{x}_{0} + \underbrace{\sqrt{\alpha_2(1 - \alpha_1)}\mathbf{\epsilon}_{0}}_{\mathbf{\epsilon}_{0}^{*}} + \underbrace{\sqrt{1 - \alpha_2}\mathbf{\epsilon}_{1}}_{\mathbf{\epsilon}_{1}^{*}}\\
&= \sqrt{\alpha_2\alpha_1}\mathbf{x}_{0} + \sqrt{1 - \alpha_1\alpha_2}\bar{\mathbf{\epsilon}}_{1} \quad\quad\quad\quad\quad\quad\quad  \text{ ;where } \bar{\mathbf{\epsilon}}_{1} \text{ merges two Gaussians (*).} \\
&= \dots \\
x_t &= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\mathbf{\epsilon}_{t} \\
q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I}) \quad\quad\quad\quad\quad\quad\quad  \text{ ; Transition density function.}
\end{aligned}
$$
</div>

<p align="justify">
(*) Recall the properties of the sum of two Gaussian random variables. Let $\mathbf{\epsilon}_{0}^{*} \sim \mathcal{N}(\mathbf{\mu}_0, \sigma^2_0\mathbf{I})$, and $\mathbf{\epsilon}_{1}^{*} \sim \mathcal{N}(\mathbf{\mu}_1, \sigma^2_1\mathbf{I})$. Then, the new random variable  $\mathbf{z}= \mathbf{\epsilon}_{0}^{*}+ \mathbf{\epsilon}_{1}^{*}$ has density $\mathcal{N}(\mathbf{\mu}_0+\mathbf{\mu}_1, (\sigma^2_0+\sigma^2_1)\mathbf{I})$. First we can de-reparametrized $\epsilon_{0}^{*}$ and  $\epsilon_{1}^{*}$ so we get $\mathcal{N}(0,Var(\epsilon_0^{*}))$ and $\mathcal{N}(0,Var(\epsilon_1^{*}))$ correspondingly. We have that the sum of these two Gaussians is
<div>
$$
\begin{aligned}
\mathbf{z}&= \mathbf{\epsilon}_{0}^{*}+ \mathbf{\epsilon}_{1}^{*}\\
&=\mathcal{N}(0,Var(\epsilon_0^{*})+ Var(\epsilon_1^{*}))\\
&=\mathcal{N}(0,(1 - \alpha_1 \alpha_2)\mathbf{I}))\quad\quad\quad  \text{ ;Using the property of the variance (*).}\\
&=\sqrt{1- \alpha_1 \alpha_2}\bar{\mathbf{\epsilon}}_{1} \quad \text{ ;where } \bar{\mathbf{\epsilon}}_{1}\sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \text{  and used reparameterization}\\
\end{aligned}
$$
</div>

(*) Recall the  recall $Var(aX) = a^2 Var(X)$. Therefore, we have the following $Var(\epsilon_0^{*})=Var(\sqrt{\alpha_2(1 - \alpha_1)}\mathbf{\epsilon}_{0})= \alpha_2(1 - \alpha_1)$. and $Var(\epsilon_1^{*})=Var(\sqrt{1 - \alpha_2}\mathbf{\epsilon}_{1})= 1 - \alpha_2$. Finally, $Var(\epsilon_0^{*})+ Var(\epsilon_1^{*})= \alpha_2(1 - \alpha_1)1 - \alpha_2= 1 - \alpha_1 \alpha_2$.
</p>


$\bar{\alpha}_t$ is an increasing function such that $\bar{\alpha}_1 > ... > \bar{\alpha}_T$. The power signal of $\mathbf{x}_0$ decreases over time, while the noise intensifies. Figure 3 shows an example of this model for 4 different 2-dimensional inputs $\mathbf{x}_0$ moving them to an Isotropic Gaussian distribution as $t\rightarrow T=1000$.




<div class="container" style="align: left; text-align:center;">
  <div class="row" >
      <div class="col-6" >
          <img class="img-fluid rounded " src="{{ site.baseurl }}/assets/img/diffusion/ddpm_animation.gif" style="width: 100%;" class="center">
      </div>
  <div class="col">
      <div class="col-10">
          <img class="img-fluid rounded" src="{{ site.baseurl }}/assets/img/diffusion/signal.png"
          style="width: 100%;">
      </div>
      <div class="col-10">
          <img class="img-fluid rounded" src="{{ site.baseurl }}/assets/img/diffusion/noise.png"
          style="width: 100%;">
      </div>
  </div>
  </div>
  <figcaption class="figure-caption text-center">Figure 3. 2D Forward Diffusion process over 1000 timesteps using equation
  \ref{eq:transition_kernel}. The forward diffusion process systematically perturbed the input data $\mathbf{x}_0$, gradually converting $q_0(\mathbf{x}_0)$ into an Isotropic Gaussian distribution for different initial states $\mathbf{x}_0$. The two plots in the right show the corresponding evolution of the <b>signal</b> and <b>noise</b> factor over time.</figcaption>
</div>

<b>In summary</b>: the forward diffusion maps any sample to a chosen stationary distribution; under this transition kernel, to an Isotropic Gaussian.
#### Reverse trajectory
 Then a generative Markov chain converts $q_T \approx p_{\theta}(\mathbf{x}_T)$, the simple distribution, into a target (data) distribution using a diffusion process. Figure 1 shows how the generative Markov chain is used to generated samples like the training distribution starting from $ x_T \sim p(x_T)$


### Continuous Diffusion Models
